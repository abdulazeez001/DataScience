{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.1"
    },
    "colab": {
      "name": "scrapping_starter-checkpoint.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3sbzEran8Hz5",
        "colab_type": "text"
      },
      "source": [
        "## Web scrapping using python\n",
        "\n",
        "#### References\n",
        "1. [Practical Introduction to Web Scraping in Python](https://realpython.com/python-web-scraping-practical-introduction/)\n",
        "2. [Web Scraping using Python](https://www.datacamp.com/community/tutorials/web-scraping-using-python)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "53_WCBGz8H0E",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# $ python3 -m venv venv\n",
        "# $ . ./venv/bin/activate"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ScezMcJi8H1u",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 323
        },
        "outputId": "e2843233-f8e8-4903-9a9c-4c9086c6ddc2"
      },
      "source": [
        "#Better\n",
        "!pip install requests BeautifulSoup4 fire"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (2.23.0)\n",
            "Requirement already satisfied: BeautifulSoup4 in /usr/local/lib/python3.6/dist-packages (4.6.3)\n",
            "Collecting fire\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/34/a7/0e22e70778aca01a52b9c899d9c145c6396d7b613719cd63db97ffa13f2f/fire-0.3.1.tar.gz (81kB)\n",
            "\r\u001b[K     |████                            | 10kB 19.7MB/s eta 0:00:01\r\u001b[K     |████████                        | 20kB 6.7MB/s eta 0:00:01\r\u001b[K     |████████████▏                   | 30kB 8.4MB/s eta 0:00:01\r\u001b[K     |████████████████▏               | 40kB 7.9MB/s eta 0:00:01\r\u001b[K     |████████████████████▏           | 51kB 6.9MB/s eta 0:00:01\r\u001b[K     |████████████████████████▎       | 61kB 7.4MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▎   | 71kB 7.8MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 81kB 4.1MB/s \n",
            "\u001b[?25hRequirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests) (2020.6.20)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from fire) (1.12.0)\n",
            "Requirement already satisfied: termcolor in /usr/local/lib/python3.6/dist-packages (from fire) (1.1.0)\n",
            "Building wheels for collected packages: fire\n",
            "  Building wheel for fire (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for fire: filename=fire-0.3.1-py2.py3-none-any.whl size=111005 sha256=c7a66d09be2f2698a70c83b88adf2e5bf4d1def2cf1090141e6447233625e640\n",
            "  Stored in directory: /root/.cache/pip/wheels/c1/61/df/768b03527bf006b546dce284eb4249b185669e65afc5fbb2ac\n",
            "Successfully built fire\n",
            "Installing collected packages: fire\n",
            "Successfully installed fire-0.3.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZCP6-KMk8H49",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from requests import get\n",
        "from requests.exceptions import RequestException\n",
        "from contextlib import closing\n",
        "from bs4 import BeautifulSoup\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os, sys\n",
        "\n",
        "import fire"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MpKn21p28H52",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#%%writefile ../pyscrap_url.py\n",
        "\n",
        "def simple_get(url):\n",
        "    \"\"\"\n",
        "    Attempts to get the content at `url` by making an HTTP GET request.\n",
        "    If the content-type of response is some kind of HTML/XML, return the\n",
        "    text content, otherwise return None.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        with closing(get(url, stream=True)) as resp:\n",
        "            if is_good_response(resp):\n",
        "                return resp.content  #.encode(BeautifulSoup.original_encoding)\n",
        "            else:\n",
        "                return None\n",
        "\n",
        "    except RequestException as e:\n",
        "        log_error('Error during requests to {0} : {1}'.format(url, str(e)))\n",
        "        return None\n",
        "\n",
        "\n",
        "def is_good_response(resp):\n",
        "    \"\"\"\n",
        "    Returns True if the response seems to be HTML, False otherwise.\n",
        "    \"\"\"\n",
        "    content_type = resp.headers['Content-Type'].lower()\n",
        "    return (resp.status_code == 200 \n",
        "            and content_type is not None \n",
        "            and content_type.find('html') > -1)\n",
        "\n",
        "\n",
        "def log_error(e):\n",
        "    \"\"\"\n",
        "    It is always a good idea to log errors. \n",
        "    This function just prints them, but you can\n",
        "    make it do anything.\n",
        "    \"\"\"\n",
        "    print(e)\n",
        "    \n",
        "def get_elements(url, tag='',search={}, fname=None):\n",
        "    \"\"\"\n",
        "    Downloads a page specified by the url parameter\n",
        "    and returns a list of strings, one per tag element\n",
        "    \"\"\"\n",
        "    \n",
        "    if isinstance(url,str):\n",
        "        response = simple_get(url)\n",
        "    else:\n",
        "        #if already it is a loaded html page\n",
        "        response = url\n",
        "\n",
        "    if response is not None:\n",
        "        html = BeautifulSoup(response, 'html.parser')\n",
        "        \n",
        "        res = []\n",
        "        if tag:    \n",
        "            for li in html.select(tag):\n",
        "                for name in li.text.split('\\n'):\n",
        "                    if len(name) > 0:\n",
        "                        res.append(name.strip())\n",
        "                       \n",
        "                \n",
        "        if search:\n",
        "            soup = html            \n",
        "            \n",
        "            \n",
        "            r = ''\n",
        "            if 'find' in search.keys():\n",
        "                print('findaing',search['find'])\n",
        "                soup = soup.find(**search['find'])\n",
        "                r = soup\n",
        "\n",
        "                \n",
        "            if 'find_all' in search.keys():\n",
        "                print('findaing all of',search['find_all'])\n",
        "                r = soup.find_all(**search['find_all'])\n",
        "   \n",
        "            if r:\n",
        "                for x in list(r):\n",
        "                    if len(x) > 0:\n",
        "                        res.extend(x)\n",
        "            \n",
        "        return res\n",
        "\n",
        "    # Raise an exception if we failed to get any data from the url\n",
        "    raise Exception('Error retrieving contents at {}'.format(url))    \n",
        "    \n",
        "    \n",
        "if get_ipython().__class__.__name__ == '__main__':\n",
        "    fire(get_tag_elements)"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "jupyter": {
          "outputs_hidden": true
        },
        "id": "KZg8F5aa8H94",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 180
        },
        "outputId": "41f27d72-a037-44ae-d24f-f01f72e59c6a"
      },
      "source": [
        "res = get_tag_elements('https://africafreak.com/100-most-influential-twitter-users-in-africa')\n",
        "res"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-8-c6059b14b637>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_tag_elements\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'https://africafreak.com/100-most-influential-twitter-users-in-africa'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mres\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'get_tag_elements' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o-GObYGB8H-X",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "url= 'https://www.atlanticcouncil.org/blogs/africasource/african-leaders-respond-to-coronavirus-on-twitter/#east-africa'\n",
        "response = simple_get(url)"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2eHYq0T_8IAH",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "eb06fa96-0e9a-4efe-f262-d97186067f18"
      },
      "source": [
        "res = get_elements(response, search={'find_all':{'class_':'wp-block-embed__wrapper'}})"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "findaing all of {'class_': 'wp-block-embed__wrapper'}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F1zfDN6C8ICc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "influencial_url= 'https://africafreak.com/100-most-influential-twitter-users-in-africa'\n",
        "inf_response = simple_get(influencial_url)"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JDrq5Fy0DP8B",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "soup = BeautifulSoup(inf_response,\"html.parser\")\n"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M5-B1MkQMEsr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "all = soup.find_all(\"h2\",{\"class\":\"\"})\n"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QIx76p3FV9rR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def print_top_ten(data):\n",
        "  top_ten=[]\n",
        "  i = -4\n",
        "  while i>-14:\n",
        "    top_ten.append(data[i].text)\n",
        "    i-=1\n",
        "  return top_ten\n",
        "top_ten = print_top_ten(all)"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zX5JFJ6FgWSf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def extract_twitter_handle(data):\n",
        "  twitter_handle = {}\n",
        "  j=1\n",
        "  for i in data:\n",
        "    twitter_handle[j]=i.split('(',1)[1].split(')')[0]\n",
        "    j+=1\n",
        "  return twitter_handle  \n",
        "\n",
        "twitter_handle = extract_twitter_handle(top_ten) "
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LhXQq8baSx-q",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "handles = pd.DataFrame.from_dict(twitter_handle,orient='index',columns=['Influentials'])"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NAcZAfz7U8ME",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "handles.to_csv('Influential.csv')"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XVciF2Wz8ICs",
        "colab_type": "text"
      },
      "source": [
        "## Web scrapping using bash script\n",
        "If the web site has a quite simple HTML, you can easily use curl to perform the request and then extract the needed values using bash commands grep, cut , sed, ..\n",
        "\n",
        "This tutorial is adapted from [this](https://medium.com/@LiliSousa/web-scraping-with-bash-690e4ee7f98d) medium article"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5QMV2BFD8ICv",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "5afe7710-3dbd-4fcc-b707-4cdc27638488"
      },
      "source": [
        "%%bash \n",
        "\n",
        "# curl the page and save content to tmp_file\n",
        "#url = \"https://www.atlanticcouncil.org/blogs/africasource/african-leaders-respond-to-coronavirus-on-twitter/#east-africa\"\n",
        "#curl -X GET $url -o tmp_file\n",
        "\n",
        "\n",
        "#!/bin/bash\n",
        "\n",
        "# write headers to CSV file\n",
        "echo \"Name, twitter_id\" >> extractData.csv\n",
        "n=\"1\"\n",
        "while [ $n -lt 2 ]\n",
        "do\n",
        "  \n",
        "  #get title\n",
        "  title=$(cat tmp_file | grep \"class=\\\"twitter-tweet\\\"\" | cut -d ';' -f1 )\n",
        "  echo $title\n",
        "  #get author\n",
        "  #twitter_id=$(cat tmp_file |grep -A1 \"class=\\\"css-901oao css-16my406 r-1qd0xha r-ad9z0x r-bcqeeo r-qvutc0\\\"\" | tail -1)\n",
        "\n",
        "  #echo \"$title, $twitter_id\" >> extractData.csv\n",
        "  #echo \"$title, $twitter_id\"\n",
        "    \n",
        "  n=$[$n+1]\n",
        "\n",
        "done"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "cat: tmp_file: No such file or directory\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n5ePqOH68IEf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HbN6PWC_gSmT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mWSrdp-6Svwk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}